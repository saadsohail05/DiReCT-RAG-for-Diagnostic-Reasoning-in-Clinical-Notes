{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/saad-sohail/2fc1c3ec-de45-4017-8a0b-677ed416cbf4/DiReCT-RAG-for-Diagnostic-Reasoning-in-Clinical-Notes/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA API key loaded successfully\n",
      "Google API key loaded successfully\n",
      "Hugging Face token loaded successfully\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"NVIDIA_API_KEY\"] = os.getenv(\"NVIDIA_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "if os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "    print(\"NVIDIA API key loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load NVIDIA API key\")\n",
    "\n",
    "if os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    print(\"Google API key loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load Google API key\")\n",
    "\n",
    "if os.environ.get(\"HUGGINGFACE_TOKEN\"):\n",
    "    print(\"Hugging Face token loaded successfully\")\n",
    "else:\n",
    "    print(\"Failed to load Hugging Face token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Ingestion Pipeline - Document Loading & Processing Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_json_files(base_path: str) -> List[str]:\n",
    "    json_files = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        if 'diagnostic_kg' in root:      # No KG files\n",
    "            continue\n",
    "        for f in files:\n",
    "            if f.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, f))\n",
    "    return json_files\n",
    "\n",
    "def read_json_file(path: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def extract_categories_from_path(path: str) -> Dict[str, str]:\n",
    "    parts = path.split(os.sep)\n",
    "    try:\n",
    "        idx = parts.index('Finished')\n",
    "        category = parts[idx+1] if len(parts) > idx+1 else \"Unknown\"\n",
    "        subcategory = parts[idx+2] if len(parts) > idx+2 else \"None\"\n",
    "        return {\n",
    "            \"category\": category,\n",
    "            \"subcategory\": subcategory\n",
    "        }\n",
    "    except ValueError:\n",
    "        return {\"category\": \"Unknown\", \"subcategory\": \"Unknown\"}\n",
    "\n",
    "def flatten_json_to_text(data: Dict[str, Any], parent_key='') -> str:\n",
    "    \"\"\"Convert nested JSON to a flattened text format optimized for clinical notes.\"\"\"\n",
    "    result = []\n",
    "    \n",
    "    # Clinical note section headers\n",
    "    section_headers = {\n",
    "        'input1': 'CHIEF COMPLAINT',\n",
    "        'input2': 'HISTORY OF PRESENT ILLNESS',\n",
    "        'input3': 'PAST MEDICAL HISTORY',\n",
    "        'input4': 'FAMILY HISTORY',\n",
    "        'input5': 'PHYSICAL EXAMINATION',\n",
    "        'input6': 'PERTINENT RESULTS'\n",
    "    }\n",
    "    \n",
    "    def _flatten(obj, prefix=''):\n",
    "        if isinstance(obj, dict):\n",
    "            for input_key in sorted(section_headers.keys()):\n",
    "                if input_key in obj and obj[input_key]:\n",
    "                    result.append(f\"\\n=== {section_headers[input_key]} ===\")\n",
    "                    result.append(obj[input_key].strip())\n",
    "            \n",
    "            for k in sorted(obj.keys()):\n",
    "                if k not in section_headers:\n",
    "                    v = obj[k]\n",
    "                    new_key = f\"{prefix}.{k}\" if prefix else k\n",
    "                    if isinstance(v, (dict, list)):\n",
    "                        _flatten(v, new_key)\n",
    "                    elif v:  \n",
    "                        result.append(f\"{new_key}: {v}\")\n",
    "        elif isinstance(obj, list):\n",
    "            for i, item in enumerate(obj):\n",
    "                _flatten(item, f\"{prefix}[{i}]\")\n",
    "        elif obj and prefix: \n",
    "            result.append(f\"{prefix}: {obj}\")\n",
    "    \n",
    "    _flatten(data)\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def create_documents_from_json(files: List[str]) -> List[Document]:\n",
    "    docs = []\n",
    "    for path in files:\n",
    "        data = read_json_file(path)\n",
    "        if not data: continue\n",
    "        \n",
    "        meta = extract_categories_from_path(path)\n",
    "        \n",
    "        flattened_text = flatten_json_to_text(data)\n",
    "        \n",
    "        header = f\"\"\"\n",
    "=== CLINICAL NOTE ===\n",
    "Category: {meta['category']}\n",
    "Subcategory: {meta['subcategory']}\n",
    "Source: {os.path.basename(path)}\n",
    "\n",
    "{flattened_text}\n",
    "\"\"\"\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=header.strip(),\n",
    "            metadata={\n",
    "                \"source\": path,\n",
    "                \"category\": meta[\"category\"],\n",
    "                \"subcategory\": meta[\"subcategory\"],\n",
    "                \"filename\": os.path.basename(path),\n",
    "                \"document_type\": \"clinical_note\"\n",
    "            }\n",
    "        )\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Document Pipeline Execution (RAG Ingestion)\n",
    "BASE_PATH = \"mimic-iv-ext-direct-1.0.0/mimic-iv-ext-direct-1.0.0/samples/Finished\"\n",
    "json_files = find_all_json_files(BASE_PATH)\n",
    "documents = create_documents_from_json(json_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents created: 511\n",
      "\n",
      "Sample document preview:\n",
      "Source: mimic-iv-ext-direct-1.0.0/mimic-iv-ext-direct-1.0.0/samples/Finished/Acute Coronary Syndrome/NSTEMI/17183564-DS-13.json\n",
      "Category: Acute Coronary Syndrome\n",
      "Content length: 2792 characters\n",
      "Content preview: === CLINICAL NOTE ===\n",
      "Category: Acute Coronary Syndrome\n",
      "Subcategory: NSTEMI\n",
      "Source: 17183564-DS-13.json\n",
      "\n",
      "\n",
      "=== CHIEF COMPLAINT ===\n",
      "Chest Pain\n",
      "\n",
      "=== HISTORY OF PRESENT ILLNESS ===\n",
      "a man with PMHx of HTN and chronic hepatitis C who presents with substernal chest pain. The patient reports that he was sitting in a recliner this AM drinking coffee, when he began to experience chest pain. The pain dissipated sponatneously after a few minutes, and then returned for approximately 30 minutes. The pain resolved immediately with nitro and aspirin administered by the EMTs. Reports tingling down both extremities and associated diaphoresis and shortness of breath. Denies nausea, vomiting, palpitations, and loss of conscioussness. The patient reports no cardiac history, and has never experienced chest pain like this before.\n",
      "\n",
      "=== PAST MEDICAL HISTORY ===\n",
      "+ HTN\n",
      "+ Chronic hepatitis C\n",
      "+ h/o H. pylori\n",
      "+ h/o bacterial PNA\n",
      "+ h/o acute pancreatitis\n",
      "+ s/p lap cholecystectomy\n",
      "\n",
      "=== FAMILY HISTORY ===\n",
      "father deceased before, h/o alcoholism and pancreatitis\n",
      "\n",
      "=== PHYSICAL EXAMINATION ===\n",
      "VS -  98.9  134/82  59  97% RA\n",
      "Gen: WDWN middle aged male in NAD. Oriented x3. Mood, affect appropriate.  \n",
      "HEENT: PERRL, EOMI.   \n",
      "Neck: Supple.\n",
      "CV: RR, normal S1, S2. No m/r/g. No thrills, lifts. No S3 or S4. \n",
      " \n",
      "Chest: Resp were unlabored, no accessory muscle use. CTAB anteriorly.\n",
      "Abd: Soft, NTND. No HSM or tenderness. \n",
      "Ext: No c/c/e. No femoral bruits. No hematoma.\n",
      "\n",
      "=== PERTINENT RESULTS ===\n",
      "Labs:\n",
      "06:20AM BLOOD WBC-5.6 RBC-4.43* Hgb-13.9* Hct-40.7 MCV-92 MCH-31.3 MCHC-34.0 RDW-13.1\n",
      "06:20AM BLOOD Neuts-53.1 Monos-4.1 Eos-1.4 Baso-1.1\n",
      "06:20AM BLOOD Glucose-120* UreaN-21* Creat-1.0 Na-140 \n",
      "K-3.9 Cl-107 HCO3-27 AnGap-10\n",
      "06:20AM BLOOD cTropnT-0.60*\n",
      "06:20AM BLOOD CK-MB-3 cTropnT-0.09*\n",
      "06:20AM BLOOD Triglyc-128 HDL-38 CHOL/HD-3.8 LDLcalc-81\n",
      "﻿\n",
      "Cath Report:\n",
      "1.  Selective coronary angiography in this right dominant system demonstrated acute thrombotic 1 vessel coronary artery disease.  The LMCA, LAD, and LCx had no angiographically apparent flow-limiting disease.  The RCA had a 90% proximal stenosis with extensive thrombus,as well as a 70% mid to distal segment stenosis.\n",
      "2.  Limited resting hemodynamics revealed mild systemic systolic arterial hypertension with an SBP of 145 mmHg.\n",
      "3.  Successful angiojet/PTCA/stenting of the mid and distal RCA: the mid-distal RCA was stented with a VISIOn 3.5x12mm bare-metal stent (BMS) and post-dilated with an NC Quantum Apex MR 3.75x20mm balloon and the mid RCA was stented with a VISION 3.5x28mm BMS and then postdilated with an NC Quantum Apex OTW 4.0x20mm balloon. Final angiography revealed 0%residual stenosis, no angiographically apparent dissection and normal flow.\n",
      "4. femoral artery angioseal closure device deployed without complications.\n",
      "\n",
      "ECG:non-ST-elevation...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total documents created: {len(documents)}\")\n",
    "if documents:\n",
    "    print(\"\\nSample document preview:\")\n",
    "    sample = documents[0]\n",
    "    print(f\"Source: {sample.metadata['source']}\")\n",
    "    print(f\"Category: {sample.metadata['category']}\")\n",
    "    print(f\"Content length: {len(sample.page_content)} characters\")\n",
    "    print(f\"Content preview: {sample.page_content}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Text Splitting - Chunking Component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1500,  \n",
    "#     chunk_overlap=300,  \n",
    "#     length_function=len,\n",
    "#     separators=[\n",
    "#         \"\\n=== \", # Split on section headers first\n",
    "#         \"\\n\\n\",   # Then paragraphs\n",
    "#         \"\\n\",     # Then lines\n",
    "#         \". \",     # Then sentences\n",
    "#         \", \",     # Then clauses\n",
    "#         \" \",      # Then words\n",
    "#         \"\"\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Filter out tiny chunks that don't contain meaningful content\n",
    "# chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# meaningful_chunks = [chunk for chunk in chunks if len(chunk.page_content.strip()) > 100]  \n",
    "\n",
    "# # Print statistics about the chunks\n",
    "# print(f\"Original documents: {len(documents)}\")\n",
    "# print(f\"Raw chunks after splitting: {len(chunks)}\")\n",
    "# print(f\"Filtered chunks (removing tiny chunks): {len(meaningful_chunks)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "semantic_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\",token=\"huggingface_token\")\n",
    "\n",
    "similarity_threshold = 0.7  # Tune this\n",
    "\n",
    "def semantic_chunk(text: str, max_chunk_size=1500) -> List[str]:\n",
    "    sentences = text.split(\". \")\n",
    "    embeddings = semantic_model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_tokens = 0\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        current_chunk.append(sentence)\n",
    "        current_chunk_tokens += len(sentence)\n",
    "\n",
    "        # Determine whether to split based on similarity to next sentence\n",
    "        if i < len(sentences) - 1:\n",
    "            sim = util.cos_sim(embeddings[i], embeddings[i + 1]).item()\n",
    "            if sim < similarity_threshold or current_chunk_tokens > max_chunk_size:\n",
    "                chunks.append(\". \".join(current_chunk).strip() + \".\")\n",
    "                current_chunk = []\n",
    "                current_chunk_tokens = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\". \".join(current_chunk).strip() + \".\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    semantic_chunks = semantic_chunk(doc.page_content)\n",
    "    for chunk_text in semantic_chunks:\n",
    "        if len(chunk_text.strip()) > 100:  # Filter tiny chunks\n",
    "            chunks.append(Document(page_content=chunk_text.strip(), metadata=doc.metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Vector Embedding - Embedding Component \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClinicalEmbeddings(Embeddings):\n",
    "#     def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "#         return [self.embed_query(text) for text in texts]\n",
    "\n",
    "#     def embed_query(self, text: str) -> List[float]:\n",
    "#         text = f\"\"\"Clinical Context:\n",
    "#         Patient Information and History: {text}\n",
    "        \n",
    "#         Analysis Framework:\n",
    "#         - Primary symptoms and presentations\n",
    "#         - Relevant medical, surgical, and family history\n",
    "#         - Physical examination findings\n",
    "#         - Diagnostic considerations (labs, imaging, specialized tests)\n",
    "#         - Differential diagnoses across medical specialties\n",
    "#         - Treatment implications and follow-up plan\"\"\"\n",
    "        \n",
    "#         inputs = tokenizer(\n",
    "#             text, \n",
    "#             return_tensors=\"pt\", \n",
    "#             padding=True, \n",
    "#             truncation=True, \n",
    "#             max_length=768  \n",
    "#         )\n",
    "#         inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             # Using mean pooling instead of just CLS token\n",
    "#             embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
    "        \n",
    "#         return embeddings.tolist()\n",
    "\n",
    "# clinical_embeddings = ClinicalEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(embedding_type: str = \"google\") -> Embeddings:\n",
    "    if embedding_type == \"google\":\n",
    "        return GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            task_type=\"retrieval_document\"\n",
    "        )\n",
    "    else:  # Default to clinical embeddings\n",
    "        # Set up Clinical ModernBERT\n",
    "        model = AutoModel.from_pretrained('Simonlee711/Clinical_ModernBERT')\n",
    "        tokenizer = AutoTokenizer.from_pretrained('Simonlee711/Clinical_ModernBERT')\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        return ClinicalEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Storage & Indexing - Vector Store Component \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_654031/2632416334.py:16: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "db_directory = \"chroma_db\"  # Directory to store Chroma database\n",
    "\n",
    "import shutil\n",
    "if os.path.exists(db_directory):\n",
    "    print(f\"Removing existing database directory: {db_directory}\")\n",
    "    shutil.rmtree(db_directory)\n",
    "\n",
    "embeddings = get_embeddings(\"google\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=db_directory\n",
    ")\n",
    "\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Retriever - Retrieval Component \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Using MMR for better diversity in results\n",
    "    search_kwargs={\n",
    "        \"k\": 5,  # Number of documents to retrieve\n",
    "        \"fetch_k\": 10,  # Fetch more documents initially for MMR to choose from\n",
    "        \"lambda_mult\": 0.7,  # Balance between relevance and diversity\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Prompt Engineering - Prompt Template Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert medical AI assistant specialized in clinical decision support. Use the following retrieved medical documents to answer the question.\n",
    "\n",
    "Here's an example of how to analyze a medical query:\n",
    "Example Query: \"I have chest pain and shortness of breath\"\n",
    "Reasoning Process:\n",
    "1. Consider urgent vs non-urgent presentation\n",
    "2. Analyze key symptoms and their patterns\n",
    "3. Review risk factors and medical history\n",
    "4. Evaluate differential diagnoses\n",
    "5. Determine appropriate level of care\n",
    "\n",
    "Example Response:\n",
    "Key Findings: Acute chest pain with dyspnea suggests cardiopulmonary etiology\n",
    "Clinical Interpretation: Given symptoms, must rule out acute coronary syndrome\n",
    "Recommendations: Immediate emergency evaluation recommended\n",
    "Chain of Thought: Chest pain + shortness of breath → possible cardiac/pulmonary cause → risk of ACS → requires urgent assessment\n",
    "\n",
    "CONTEXT DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "CLINICAL QUERY: {question}\n",
    "\n",
    "Please provide a detailed medical response following this structure:\n",
    "1. Key Findings: Summarize the most relevant information\n",
    "2. Clinical Interpretation: Analyze the medical significance\n",
    "3. Chain of Thought: Show your reasoning process step by step\n",
    "4. Recommendations: Suggest evidence-based approaches\n",
    "5. References: Cite specific sections from the provided context\n",
    "\n",
    "If you cannot provide a complete answer based on the available context, explicitly state the limitations.\n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document formatting with clinical section emphasis\n",
    "def format_docs(docs):\n",
    "    formatted_sections = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        # Extract key clinical sections\n",
    "        sections = doc.page_content.split(\"===\")\n",
    "        formatted = f\"\\nSOURCE {i}:\\n\"\n",
    "        formatted += f\"Category: {doc.metadata['category']}\\n\"\n",
    "        formatted += f\"Document Type: {doc.metadata['document_type']}\\n\"\n",
    "        formatted += \"-\" * 40 + \"\\n\"\n",
    "        formatted += \"\\n\".join(section.strip() for section in sections if section.strip())\n",
    "        formatted_sections.append(formatted)\n",
    "    return \"\\n\\n\".join(formatted_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Generator/LLM - Generation Component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "# Palmyra-Med-70B-32k\n",
    "llm = ChatNVIDIA(model=\"writer/palmyra-med-70b-32k\")\n",
    "\n",
    "# Create the RAG prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Orchestration - RAG Pipeline Integration Component \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Query Handling - Query Processing Component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_medical_query(query: str, min_sources: int = 5, confidence_threshold: float = 0.7) -> dict:\n",
    "    try:\n",
    "        # Get retrieved documents and their similarity scores (distances)\n",
    "        retrieved_docs_with_scores = retriever.vectorstore.similarity_search_with_score(query, k=min_sources)\n",
    "        \n",
    "        # Check source adequacy\n",
    "        if len(retrieved_docs_with_scores) < min_sources:\n",
    "            return {\n",
    "                \"answer\": \"Insufficient clinical evidence found. Please refine the query.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"warnings\": [\"Insufficient source documents found\"]\n",
    "            }\n",
    "\n",
    "        # Separate documents and distances (lower = more similar)\n",
    "        retrieved_docs = [doc for doc, distance in retrieved_docs_with_scores]\n",
    "        distances = [distance for doc, distance in retrieved_docs_with_scores]\n",
    "\n",
    "        # Enhanced confidence scoring with dynamic thresholding\n",
    "        similarity_scores = [1 / (1 + d) for d in distances]\n",
    "        avg_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "        std_similarity = (sum((s - avg_similarity) ** 2 for s in similarity_scores) / len(similarity_scores)) ** 0.5\n",
    "        \n",
    "        # Adjust confidence threshold based on query complexity\n",
    "        adjusted_threshold = confidence_threshold - (0.1 if len(query.split()) > 10 else 0)\n",
    "        \n",
    "        # Calculate final confidence score\n",
    "        confidence = avg_similarity * (1 - std_similarity)  # Penalize high variance\n",
    "        \n",
    "        # Generate response\n",
    "        response = rag_chain.invoke(query)\n",
    "\n",
    "        # Enhanced warning system\n",
    "        warnings = []\n",
    "        if confidence < adjusted_threshold:\n",
    "            warnings.append(\"Low confidence response - please verify with healthcare provider\")\n",
    "        if std_similarity > 0.3:  # High variance in relevance\n",
    "            warnings.append(\"Mixed relevance in sources - interpretation may be limited\")\n",
    "\n",
    "        return {\n",
    "            \"answer\": response,\n",
    "            \"sources\": [doc.metadata.get(\"source\", \"unknown\") for doc in retrieved_docs],\n",
    "            \"confidence\": round(confidence, 2),\n",
    "            \"reasoning_confidence\": round(avg_similarity, 2),\n",
    "            \"source_consistency\": round(1 - std_similarity, 2),\n",
    "            \"warnings\": warnings\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"Error processing clinical query: {str(e)}\",\n",
    "            \"sources\": [],\n",
    "            \"confidence\": 0.0,\n",
    "            \"warnings\": [\"Processing error occurred\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running clinical test queries:\n",
      "\n",
      "CLINICAL QUERY: I have a headache and nausea from past week. What could be the cause?\n",
      "\n",
      "RESPONSE:\n",
      "Query Analysis:\n",
      "\n",
      "Key Findings: Headache and nausea from past week. No mention of fever or trauma to the head.\n",
      "Clinical Interpretation: Migraine or tension headaches could be the cause of persistent headaches. Presence of nausea may suggest involvement of the gastrointestinal system. Hormonal imbalance and zygomatic sinusitis can be less likely causes.\n",
      "Chain of Thought: 1. Headache + nausea → Rule out causes of tension headache, migraine, and sinusitis.\n",
      "2. Is associated with trauma or fever? → If yes, sinus infection or subdural hematoma should be considered.\n",
      "3. Duration of symptoms ≥ 1 week → Could be a migraine or tension headache if not improving with self-care.\n",
      "4. Check previous medical history for hormonal imbalances, thyroid disorders, and migraines which might require specific management.\n",
      "Recommendations: 1. Initial management of tension headaches includes rest, analgesics, and lifestyle modifications.\n",
      "2. Migraine prevention and exacerbation management require lifestyle modifications, prophylaxis, and acute treatment.\n",
      "References:\n",
      "1. SOURCE 2: Headache + recent behavioral changes raise the possibility of migraines and/or secondary headaches.\n",
      "2. SOURCE 5: Nausea without epigastric abdominal pain is less likely due to gastrointestinal causes plotted for glandular fever.\n",
      "\n",
      "Please consult with your healthcare provider for detailed evaluation, diagnosis, and specific guidance.\n",
      "\n",
      "SOURCES USED: 5\n",
      "CONFIDENCE SCORE: 0.80\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CLINICAL QUERY: I’ve had lower back pain that radiates down my leg. Is this a sign of a nerve issue?\n",
      "\n",
      "RESPONSE:\n",
      "Query Analysis: Lower back pain radiating down the leg is significant and requires evaluation for nerve issues within the low back lumbar spine. Current clinical query resembles symptoms in the migraines category, where similar issues were noted, incorporating low back pain radiating down the leg, even in conjunction with particulars such as numbness or asleep feelings in the leg.\n",
      "\n",
      "SOURCES USED: 5\n",
      "CONFIDENCE SCORE: 0.74\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CLINICAL QUERY: I feel dizzy when I stand up and sometimes my vision gets blurry. Should I be concerned?\n",
      "\n",
      "RESPONSE:\n",
      "Query Analysis:\n",
      "\n",
      "1. Key Findings: Dizziness and/or lightheadedness on standing, which improves once seated or with changing environmental factors like temperature, and frequently associated with visual disturbances such as seeing 'black dots'.\n",
      "2. Clinical Interpretation: Orthostatic hypotension typically causes dizziness and/or lightheadedness upon standing due to low blood pressure. Visual disturbances are likely due to hypoxia and cerebral hypoperfusion.\n",
      "3. Chain of Thought:\n",
      "a. Dizziness on standing could be due to changing blood pressure or volume on posture change.\n",
      "b. Severity of symptoms suggests a consistent cause, possibly a treatable condition.\n",
      "c. Hypothesis of hypovolemia or anemia triggers consideration of bleeding or dehydration as potential causes.\n",
      "d. Intermittent nature of symptoms may indicate an autonomic issue.\n",
      "4. Recommendations: Consult a healthcare provider to evaluate your blood pressure and volume status. Medical history should be taken to probe for any potential underlying causes. Basic laboratory evaluation, including a complete blood count to check for anemia, is advised. If preliminary examination suggests no other cause, further testing such as orthostatic hypotension diagnostic tests, tilt table testing, and hemodynamic assessments may be considered.\n",
      "5. References: Document\n",
      "\n",
      "SOURCES USED: 5\n",
      "CONFIDENCE SCORE: 0.78\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CLINICAL QUERY: My child has a sore throat and high fever. Could it be strep or something else?\n",
      "\n",
      "RESPONSE:\n",
      "KEY FINDINGS: Sore throat and high fever, possible reasons for this presentation include a bacterial/pharyngitis syndrome such as streptococcal pharyngitis.\n",
      "CLINICAL INTERPRETATION: Would expect additional symptoms to be present in a strep throat such as siblings with strep, other persons having similar symptoms, recent strep illness.\n",
      "CHAIN OF THOUGHT: Sore throat and fever are presenting symptoms, consider bacterial/pharyngitis such as Group A streptococcal (GAS) if other signs are present such as cervical adenopathy, or if siblings have GAS illness.\n",
      "RECOMMENDATIONS: Evidence-based approach would be a rapid strep test (RST), with good sensitivity and specificity, to decide if GAS is the likely cause.\n",
      "MODIFIERS: Patients who are allergic to penicillin or have not responded to previous penicillin therapy.\n",
      "REFERENCES: Recommendations based on the CDC guidelines, Group A Streptococcal Infections, clinical practice guidelines, 2012.\n",
      "\n",
      "SOURCES USED: 5\n",
      "CONFIDENCE SCORE: 0.72\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CLINICAL QUERY: I’ve noticed blood in my stool. Should I be worried about colon cancer?\n",
      "\n",
      "RESPONSE:\n",
      "Query: I’ve noticed blood in my stool. Should I be worried about colon cancer?\n",
      "Key Findings: Blood presence in stool is often due to upper gastrointestinal tract bleeding caused by conditions like gastritis or peptic ulcer disease, but it may also be a symptom of lower gastrointestinal tract issues potentially related to colon cancer.\n",
      "Clinical Interpretation: While colon cancer should be ruled out, it is not the most common cause of blood in stool. Upper gastrointestinal tract bleeding should be considered more probable if not ruled out by endoscopy.\n",
      "Chain of Thought: \n",
      "1. Is the blood in stool due to upper or lower gastrointestinal bleeding?\n",
      "2. Is upper gastrointestinal bleeding more likely than lower tract bleeding?\n",
      "3. Consider gastritis, peptic ulcer disease, and gastroesophageal reflux disease.\n",
      "4. Drops in hemoglobin or positive fecal occult blood tests suggest ongoing bleeding.\n",
      "5. The presence of upper gastrointestinal tract symptoms like nausea, vomiting, abdominal pain, and bloody emesis supports upper tract bleeding.\n",
      "6. Colon cancer may be considered if there is clinical suspicion, but only after excluding more probable upper gastrointestinal conditions.\n",
      "7. Endoscopy and blood workup are crucial for a definitive diagnosis.\n",
      "Recommendations:\n",
      "1. Urgent medical evaluation is recommended if there are concurrent symptoms like severe worsening abdominal pain, significant bloody stools, weight loss, or positive for fecal occult blood.\n",
      "2. A complete blood count, including hemoglobin levels, and an endoscopy are suggested to distinguish between upper tract and lower tract bleeding.\n",
      "3. Early referral to a gastroenterologist is recommended for further diagnostic workup and management.\n",
      "References: \n",
      "Dicuments 1, 2, and 5 for upper gastrointestinal bleed symptoms.\n",
      "Documents 3 and 4 for nausea, vomiting, and abdominal pain related to upper gastrointestinal tract bleeding.\n",
      "Link to state-of-the-art GI tract bleeding guidelines.\n",
      "\n",
      "SOURCES USED: 5\n",
      "CONFIDENCE SCORE: 0.74\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"I have a headache and nausea from past week. What could be the cause?\",\n",
    "    \"I’ve had lower back pain that radiates down my leg. Is this a sign of a nerve issue?\",\n",
    "    \"I feel dizzy when I stand up and sometimes my vision gets blurry. Should I be concerned?\",\n",
    "    \"My child has a sore throat and high fever. Could it be strep or something else?\",\n",
    "    \"I’ve noticed blood in my stool. Should I be worried about colon cancer?\",\n",
    "]\n",
    "\n",
    "print(\"\\nRunning clinical test queries:\")\n",
    "for query in test_queries:\n",
    "    print(f\"\\nCLINICAL QUERY: {query}\")\n",
    "    result = process_medical_query(query)\n",
    "    print(\"\\nRESPONSE:\")\n",
    "    print(result[\"answer\"])\n",
    "    print(\"\\nSOURCES USED:\", len(result[\"sources\"]))\n",
    "    print(\"CONFIDENCE SCORE:\", f\"{result['confidence']:.2f}\")\n",
    "    if result[\"warnings\"]:\n",
    "        print(\"WARNINGS:\", \", \".join(result[\"warnings\"]))\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('.')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 100 clinical notes.\n",
      "\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Notes: 100%|██████████| 100/100 [16:03<00:00,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Summary (100 samples) ===\n",
      "Mean Relevance Score (1–5): 3.41\n",
      "Mean Model Confidence     : 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === LLM Setup ===\n",
    "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", temperature=0)\n",
    "\n",
    "def judge_relevance(note: str, answer: str) -> int:\n",
    "    \"\"\"\n",
    "    Ask Gemini to rate how well the RAG-generated answer matches the content of the note.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a clinical QA evaluator.\n",
    "\n",
    "Clinical Note:\n",
    "{note}\n",
    "\n",
    "RAG-Generated Answer:\n",
    "{answer}\n",
    "\n",
    "Question: Based only on the information in the clinical note, how relevant and accurate is this answer?\n",
    "Rate from 1 (irrelevant/incorrect) to 5 (very accurate and relevant).\n",
    "Output only the integer.\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = gemini_llm.invoke(prompt)\n",
    "        return int(resp.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"[Gemini Parse Error] {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "BASE_PATH = \"mimic-iv-ext-direct-1.0.0/mimic-iv-ext-direct-1.0.0/samples/Finished\"\n",
    "MAX_DOCS = 100\n",
    "\n",
    "print(\"Loading documents...\")\n",
    "json_files = find_all_json_files(BASE_PATH)\n",
    "documents = create_documents_from_json(json_files)\n",
    "\n",
    "if MAX_DOCS and MAX_DOCS > 0:\n",
    "    documents = documents[:MAX_DOCS]\n",
    "\n",
    "print(f\"Loaded {len(documents)} clinical notes.\\n\")\n",
    "\n",
    "results: List[Dict[str, Any]] = []\n",
    "\n",
    "print(\"Evaluating...\\n\")\n",
    "for doc in tqdm(documents, desc=\"Evaluating Notes\"):\n",
    "    note_text = doc.page_content\n",
    "    filename = doc.metadata.get(\"filename\", \"unknown.json\")\n",
    "\n",
    "    try:\n",
    "        rag_out = process_medical_query(note_text, min_sources=5)\n",
    "        answer = rag_out.get(\"answer\", \"\")\n",
    "        confidence = rag_out.get(\"confidence\", None)\n",
    "        relevance_score = judge_relevance(note_text, answer)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed on {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    results.append({\n",
    "        \"file\": filename,\n",
    "        \"category\": doc.metadata.get(\"category\"),\n",
    "        \"subcategory\": doc.metadata.get(\"subcategory\"),\n",
    "        \"relevance_score\": relevance_score,\n",
    "        \"confidence\": confidence,\n",
    "        \"answer\": answer,\n",
    "    })\n",
    "\n",
    "if results:\n",
    "    avg_relevance = np.mean([r[\"relevance_score\"] for r in results])\n",
    "    confidences = [r[\"confidence\"] for r in results if r[\"confidence\"] is not None]\n",
    "    avg_confidence = np.mean(confidences) if confidences else None\n",
    "\n",
    "    print(f\"\\n=== Evaluation Summary ({len(results)} samples) ===\")\n",
    "    print(f\"Mean Relevance Score (1–5): {avg_relevance:.2f}\")\n",
    "    if avg_confidence is not None:\n",
    "        print(f\"Mean Model Confidence     : {avg_confidence:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo successful evaluations.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
